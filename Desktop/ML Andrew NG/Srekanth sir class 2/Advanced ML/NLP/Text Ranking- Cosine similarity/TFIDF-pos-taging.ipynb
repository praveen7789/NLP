{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./text','r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virat smashed another ton\\nworld cup start at 30th may 2019 in english\\nnarendera modi visits andhra pradesh for election summit\\ncurrently phase 9 election is completed in west bangal\\ntemperature in india turning 50 due to global warming\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " Q = 'when worlscup starts ?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding \n",
    " - Bag of words\n",
    " - TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Word Embedding\n",
    " - Bag of Word(BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning\n",
    "- converting in to lower\n",
    "- remove special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data.split('\\n')\n",
    "df = pd.DataFrame(docs,columns=['Documents'])\n",
    "df = df.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    " - Lower\n",
    " - remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textprocess(data):\n",
    "    data = data.lower() # converting in to lower case\n",
    "    data = re.sub(r'[^a-z0-9]+',' ',data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Documents'] = df['Documents'].apply(textprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virat smashed another ton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world cup start at 30th may 2019 in english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>narendera modi visits andhra pradesh for elect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>currently phase 9 election is completed in wes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temperature in india turning 50 due to global ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Documents\n",
       "0                          virat smashed another ton\n",
       "1        world cup start at 30th may 2019 in english\n",
       "2  narendera modi visits andhra pradesh for elect...\n",
       "3  currently phase 9 election is completed in wes...\n",
       "4  temperature in india turning 50 due to global ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemma \n",
    " - taking root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk import pos_tag # parts of speach taging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lema = wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virat smashed another ton'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = df['Documents'].loc[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('virat', 'NN'), ('smashed', 'VBD'), ('another', 'DT'), ('ton', 'NN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokens)  # tokens are known as words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective\t'big'\n",
    "# JJR\tadjective, comparative\t'bigger'\n",
    "# JJS\tadjective, superlative\t'biggest'\n",
    "# LS\tlist marker\t1)\n",
    "# MD\tmodal\tcould, will\n",
    "# NN\tnoun, singular 'desk'\n",
    "# NNS\tnoun plural\t'desks'\n",
    "# NNP\tproper noun, singular\t'Harrison'\n",
    "# NNPS\tproper noun, plural\t'Americans'\n",
    "# PDT\tpredeterminer\t'all the kids'\n",
    "# POS\tpossessive ending\tparent's\n",
    "# PRP\tpersonal pronoun\tI, he, she\n",
    "# PRP\tpossessive pronoun\tmy, his, hers\n",
    "# RB\tadverb\tvery, silently,\n",
    "# RBR\tadverb, comparative\tbetter\n",
    "# RBS\tadverb, superlative\tbest\n",
    "# RP\tparticle\tgive up\n",
    "# TO\tto\tgo 'to' the store.\n",
    "# UH\tinterjection\terrrrrrrrm\n",
    "# VB\tverb, base form\ttake\n",
    "# VBD\tverb, past tense\ttook\n",
    "# VBG\tverb, gerund/present participle\ttaking\n",
    "# VBN\tverb, past participle\ttaken\n",
    "# VBP\tverb, sing. present, non-3d\ttake\n",
    "# VBZ\tverb, 3rd person sing. present\ttakes\n",
    "# WDT\twh-determiner\twhich\n",
    "# WP\twh-pronoun\twho, what\n",
    "# WP$\tpossessive wh-pronoun\twhose\n",
    "$# WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/praveen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize('are',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smashed'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema.lemmatize('smashed',pos='n') # nown returns same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wordnet.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns = ['words','pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poc1 = []\n",
    "# for i in df['pos']:\n",
    "#     if i == 'NN':\n",
    "#         poc1.append('n')\n",
    "#     elif i =='ADj':\n",
    "#         poc1.append('a')\n",
    "#     elif i =='ADV':\n",
    "#         poc1.append('r')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another method sirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(pos_tag):\n",
    "    word,pos=pos_tag\n",
    "    if pos.startswith('R'):\n",
    "        pos = 'r'\n",
    "    elif pos.startswith('V'):\n",
    "        pos = 'v'\n",
    "    elif pos.startswith('J'):\n",
    "        pos ='a'\n",
    "    else:\n",
    "        pos='n'\n",
    "    return lema.lemmatize(word,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinlema(sent):\n",
    "    tokens = sent.split()\n",
    "    pos = pos_tag(tokens)\n",
    "    return \" \".join([lemmatizer(tag) for tag in pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm'] = df['Documents'].apply(joinlema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documents</th>\n",
       "      <th>lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virat smashed another ton</td>\n",
       "      <td>virat smash another ton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world cup start at 30th may 2019 in english</td>\n",
       "      <td>world cup start at 30th may 2019 in english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>narendera modi visits andhra pradesh for elect...</td>\n",
       "      <td>narendera modi visit andhra pradesh for electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>currently phase 9 election is completed in wes...</td>\n",
       "      <td>currently phase 9 election be complete in west...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temperature in india turning 50 due to global ...</td>\n",
       "      <td>temperature in india turn 50 due to global war...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Documents  \\\n",
       "0                          virat smashed another ton   \n",
       "1        world cup start at 30th may 2019 in english   \n",
       "2  narendera modi visits andhra pradesh for elect...   \n",
       "3  currently phase 9 election is completed in wes...   \n",
       "4  temperature in india turning 50 due to global ...   \n",
       "\n",
       "                                                lemm  \n",
       "0                            virat smash another ton  \n",
       "1        world cup start at 30th may 2019 in english  \n",
       "2  narendera modi visit andhra pradesh for electi...  \n",
       "3  currently phase 9 election be complete in west...  \n",
       "4  temperature in india turn 50 due to global war...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "$tfidf = log(TF+1) * log(\\frac{N}{DF+1})$\n",
    " - TF = Term Frequency\n",
    " - DF = Document Frequency\n",
    " - N = Number of times word is repeated in entire corpus\n",
    "- term=word\n",
    "### +1 is added because the value should not be == infinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virat smashed another ton\n",
      "world cup start at 30th may 2019 in english\n",
      "narendera modi visits andhra pradesh for election summit\n",
      "currently phase 9 election is completed in west bangal\n",
      "temperature in india turning 50 due to global warming\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insted of rempoving stopwords we apply TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df['lemm']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 'when world cup starts?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = textprocess(Q)\n",
    "query = joinlema(query)\n",
    "query_vector = tfidf.transform([query]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 35)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.      ],\n",
       "       [0.595896],\n",
       "       [0.      ],\n",
       "       [0.      ],\n",
       "       [0.      ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X,query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem with steming is // it converts another to anoth which is not there in dictionary "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text process\n",
    " - remove special characters\n",
    " - lower or upper \n",
    " - lemma\n",
    " - stemming\n",
    "- TFIDF , Count vectorizer== Bag of words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dimension Reduction  = hashing Trick 'or' PCA \n",
    "- apply cosine similarity  (select top 10)\n",
    "- 2nd model navi base\n",
    "models(ML,cosine,distance,DL,RNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
